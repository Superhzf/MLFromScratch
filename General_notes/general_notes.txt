1.Q: Why vectorization is faster than loops?
A:

2. Q: Difference between multiplication functions in NumPy?
A: np.multiply(a, b), element-wise multiplication
np.dot(a, b) == np.matmul(a, b) == a @ b, matrix multiplication

3. Q: What is the difference between NumPy max and NumPy maximum?
A: np.max only works on a single input array and finds the value of maximum
element in that entire array. Alternatively, it takes an axis argument and will
find the maximum value along axis of the input array.
np.maximum is to take two arrays and compute their element-wise maximum

4. Q: What is the relationship between OLS and gradient descent?
A: OLS is an analytical approach which means the problem has an closed form solution,
whereas gradient descent is an iterative method.

5. Q: Why is gradient descent often used compared to OLS?
A: Because 1. some problems do not have a closed form solution. 2. gradient descent
is computationally cheaper, in order to solve a problem using OLS, we may have to
calculate the matrix multiplication and invert matrix, they are very expensive.
3. gradient descent does not need to meet strict assumptions like no multicollinearity.

6. Q: Why is second-order optimization algorithms, such as Newton's method not as
widely used as stochastic gradient descent in machine learning problems?
A: TODO

7. I tested NAG optimization with the help of PyTorch (https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)
Assuming the PyTorch implement is correct, SGD with nesterov=True is very unstable. The loss changes drastically.
Whereas, the regular SGD with momentum works pretty well. So I decided not to implement
the NAG algorithm.
One good reference: https://fyubang.com/2019/08/10/optimizer_sgd/.
It is said that SGD with NAG is well suited for RNN, I will test this when I come to
RNN

8. Q: What is the difference between local maxima/minima and saddle point?
ref :https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/maximums-minimums-and-saddle-points
ref: https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/second-partial-derivative-test
A: When the function is continuous and differentiable, points where make the first
order derivative equal to zero are stable points, critical points, and stationary points.
When the second order is not zero they are local maxima/minima.
Inflection points are those make the second order derivative equal to zero.
Saddle points are specific for multivariable functions, when the first order derivative
is zero and the second order derivative is not zero, at the same time, different
axes disagree over whether this input should be a maximum or minimum. For example,
f = x^2+y^2, (0, 0) makes the first derivative zero and the second derivative not zero.
However, when I move in the x direction, (0,0) is the local minimum, but if I move in
the y direction, (0, 0) is the local maximum, so x and y axes disagree with each
other.

Formally, x is a minimum point of f it there is some small (ball shaped) region
in the input space around the point x, such that the highest possible value you can
get for f evaluated on points in that region is achieved at the point x.

To determine whether a point is a local minimum/maximum or saddle point, for example if
we have two variables, first find points which make the first order derivative zero,
then calculate H = Fxx*Fyy - Fxy^2, if H < 0, then it's a saddle point, if H > 0,
if fxx < 0, it's a local maximum point else it's a local minimum point. For example,
F = x^2 + y ^ 2 + pxy and point (0, 0), when p = 4, if |p| > 2, (0, 0) is a saddle
point, else (0, 0) is a global minimum.

9. Q: Maximize F = (x^2)*y, s.t. x^2 + y^2 = b
A: One solution is replace x^2 with 1 - y^2 and let the first order derivative equal to 0.
But it does not work if functions are complicated. ref: http://www.economicsdiscussion.net/essays/economics/constrained-optimisation-substitution-method-lagrange-multiplier-technique-and-lagrangian-multiplier/865
The second solution is using Lagrange multiplier. This is a classic constrained
optimization problem.The idea is if we draw the contour lines of f = (x^2)*y, those
contour lines will interact with x^2+y^1=1, after checking out different contour
lines, it is obvious that the tangent contour lines give us the largest f value.
It means the gradient at that point is perpendicular to both curves (f and x^2+y^2=1).
The gradient of f and the gradient of x^2+y^2 are proportional.
Therefore, we have:
If we let G = x^2+y^2, F = (x^2)*y, then
deltaF(Xm, Ym) = lambda*deltaG(Xm, Ym), lambda is a Lagrange multiplier.
This equation represents when the contour line of one function is tangent to the
contour line of the other.

delta(Xm, Ym) = lambda*delta(Xm, Ym), x^2+y^2=1 <=> 2xy = lambda*2x, x^2 = lambda * 2y, x^2+y^2=1
<=> x = sqrt(2/3), y = sqrt(1/3)

Repackage it, we get The Lagrangian function: L(x, y, lambda) = F - lambda * (G - b),
if we calculate all the partial derivatives of L and let them equal to 0,
then we get the same thing as above. Now, L becomes an unconstrained problem! We
could just set the gradient to 0.

10: Q: What is the physical interpretation of the Lagrange multiplier?
A: Suppose we want to maximize F = (x^2)*y s.t. x^2+y^2 = b, them lambda means that
increasing b by 1 unit, then F will be increased by lambda units.
ref. about Lagrange multiplier: https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives

11. Q: What if we have multiple (more than 1) constraints?
A: The Lagrange multiplier method can be generalized into unlimited number of
constraints. For example, please refer to multiLarange.pdf

12. Q: What is the advantage of LogSoftMax over the regular SoftMax?
A: LogSoftMax + mean (in PyTorch it is nn.NLLoss) == SoftMax + cross entropy loss.
So, LogSoftMax is equal to SoftMax + cross entropy loss numerically. Whereas, doing
these two operations separately is slow and not numerically stable per PyTorch docs.
But I don't understand why Logsoftmax is more stable. It is said that if we calculate
as a whole, the middle stages would not be saved.

13. Q: What does one-to-one, one-to-many, many-to-many, many-to-one mean in RNN/LSTM?
A: One to one is the basic machine learning problems, for example, image classification.
One to many has one input and a sequence output, for example image captioning takes
an image and outputs a sentence of words. Many to many is like machine translation,
for example the input is Chinese and we want English. Many to one is also a standard
machine learning problem, for example the input is a sentence and we want to classify
it as positive or negative.

14. Q: How does embedding works?
A: Essentially, word embedding is a lookup table. For example,

import numpy as np
# Suppose we have 5 categories for the variable weekday
vocab_size = 5
# Suppose we want to transform from 5 one-hot encoding variables to 3 variables
n_out = 3
W = np.random.rand(vocab_size, n_out)

# Suppose we have 100 observations, and the variable is weekday
X = np.random.randint(low=0, high=5, size=(100,1))

# Do transformation:
W[X]

# Suppose we have 4 observations, each observation has various length of sentence, totally 5 words
X = [np.array([0,1,2,3]),
     np.array([1,4,2,0,2]),
     np.array([3,4,1]),
     np.array([1,2,3,4,1,2,3])]
output = np.array([W[x].sum(axis=0) for x in X])[:, None, :]

15. Q: Why do embedding layers should be put in the very first positions?
A: Because we cannot calculate the gradient w.r.t the input since the input is
just the indices.

15. Why does LSTM can deal with gradient vanishing (cannot handle gradient exploding)?
A: LSTM is invented specifically to avoid the RNN gradient vanishing problem.
Basically, LSTM changes the gradient of loss w.r.t. the hidden state from multiplication
to addition. So, it is unlikely that the gradients would be small, however, it could
be large.

16. Why use negative sampling/Noise Contrastive Estimation (NCE) in word2vec?
A: Traditionally, if we want to do a multiple-class classification problem, softmax
tends to be the last layer. Basically, softmax helps us normalize the final
output so that they sum up to 1. The problem is that if the total possible classes
is very large, for example, the vocabulary size of NLP problems could be huge, then
the calculation of denominator is expensive. NCE is invented to solve this problem

17. What is the difference between negative sampling and NCE in word2vec?
A: Both negative sampling and NCE are used to solve the problem that a huge vocabulary
size could slow the calculation of softmax.

The idea of negative sampling is that
instead of summing over the probabilities of every incorrect vocabulary word in
the denominator, just pick a few. Therefore, in the training stage, make predictions
for the target word and a random sample of the other words and pretend that
represents the entire vocabulary.

One added advantage of negative sampling is that we do not have to update weights
for every vocabulary word, but instead only the weights for the target word
and a few negative samples.

references for NCE:
https://datascience.stackexchange.com/a/17905
https://leimao.github.io/article/Noise-Contrastive-Estimation/
The basic idea of NCE is that it changes the problem type from a multiple-class
classification to a binary classification problem by sampling from noise
distributions. To be specific, with the same context h, we have a target from
the corpus and incorrect targets from a noise distribution. The object of NCE
loss is to maximize the expected value of likelihood under the correct corpus
distribution using logistic regression.

18. Why does NCE work?
A: Based on MLE of softmax function, as the ratio of noise samples to observations
from dataset k increases, the NCE gradient approaches MLE gradient.

19. What is the formula of NCE loss?
A: S_delta = S(target, context) - log((n/m)*P(target))
where
S(target, context) = X @ weight[target] + b
S(target, context) = X @ weight[neg_samples] + b
P(target) = probs[target] or
P(target) = probs[neg_samples]
probs are noise probabilities.

Loss = 1/m*(SIGMA(1,m)log(sigma(S_delta)) + SIGMA(1,n)log(1-sigma(S_delta)))
(it is logloss)
where m: number of samples, n: number of negative samples, k = n/m means
that 1 positive sample corresponds to k negative samples.


20. What is the advantages of VAE?
A: VAE has a latent variable, based on that latent variable, the output of VAE
models will be pretty much like but not exactly the same as the input. You can
control what you want

21. What are the problems of VAE?
A: The VAE model does not try to generate new images.

22. What is the idea of KL divergence?
A: Another name of KL divergence is relative entropy because we use entropy
to measure the distance of two probabilities. How? An intuitive way is to
calculate KL(p||q) is calculating the difference between the entropy of q and
the entropy of p. But actually, the formula is KL(p||q) = -SIGMA(-plogp-(-plogq))
KL(p||q) != KL(q||p). It is the relative entropy.

KL(q||p) is often used in situation where p is some unknown true distribution
and q is a proxy distribution that we are using to estimate p. Therefore, the
intuition is the expected information needed if data x is generated by p
distribution but you use q to approximate it. This ensures that it is always
larger than 0 because whenever p is the real distribution, you always need
more information.
ref: https://stats.stackexchange.com/questions/299176/where-does-the-kullback-leibler-come-from

22. What is the motivation of variational inference?
A: Based on the formula P(Z|X) = P(XZ)/P(X), if we want to get the conditional
distribution P(Z|X), in reality it is easy to get the joint distribution P(XZ),
but it could be hard to get the prior P(X) because P(X) might be a very complex.
integral. Now, the problem becomes whether somehow we can get the conditional
distribution P(Z|X) by using the join distribution P(XZ) without needing
the prior P(X). According to my description in the question 23, variational
inference finds q(Z) that maximizes the lower bound.

Variational inference is one of the solutions to this problem. Other solutions
include
* Metropolis–Hastings algorithm:
  - It is a solution based on sampling
  - More accurate
  - Takes longer to compute
  - Easy to understand
* Variational Inference:
  - It is a deterministic solution
  - Less accurate
  - Takes less time to compute
  - Hard to understand
* Laplace Approximation:
 - Very poor solution

23. What is the relationship between logP(X) and KL divergence?
A: In the formula P(Z|X) = P(XZ)/P(X), in order to get P(Z|X) based P(XZ), we
will approximate P(Z|X) using q(Z) which is a probability distribution function.
For sure, we do not know what q(Z) is as well. But, we can use KL divergence
to measure whether we have a good approximation, if it is a good estimation, then
the KL divergence should be small. So, the equation could be as follows:
KL(q(Z)||p(Z|X)) = - SIMGMA(q(Z)*log(P(Z|X)/q(Z))). Based on the formula
P(Z|X) = P(XZ)/P(X), plug this in. By doing some math, I get
-------------------------------------------
KL + SIGMA(q(Z)*log(P(XZ)/q(Z))) = logP(X) | This is very important!
-------------------------------------------
KL + LowerBound = logP(X). This is where the lower bound of variational inference
comes from.

The idea here is that P(X) is a prior, in other words, X is fixed. So logP(X)
cannot be changed. So, in order for KL to be as small as possible (ideally zero),
we can maximize the lower bound!

The reason to deal with the lower bound instead of directly the KL divergence is
because in reality it is easy to get P(XZ) but hard to get P(Z|X).


24. How to find the best q(Z) / How to maximize the lower bound?
A: Let's say we have two sets of variables
Z = {z1, z2, z3} and X = {x1, x2, x3} (the size of Z and X could be anything).
They have a joint PDF P(XZ) = P(x1, x2, x3, z1, z2, z3). Now, we want to
find the conditional probability P(Z|X) == P(z1, z3, z3 | x1, x2, x3), since it
is hard to get the prior P(x1x2x3), We will use variational inference to solve
this problem:
We want to know: P(Z|X)
We already know: P(x1, x2, x3, z1, z2, z3).
We will use q(z1, z2, z3) to estimate P(Z|X).

LowerBound = SIGMAz1z2z3(q(z1z2z3)*log(P(x1x2x3z1z2z3)/q(z1z2z3))), the goal is to
maximize this and find what q(z1z2z3) is.

This is hard, so we make some assumptions on this:
* Let's assume z1, z2 and z3 are independent, the q(z1z2z3) = q(z1)q(z2)q(z3)
Then, plug this in, then
LowerBound = SIGMAz1z2z3(q(z1)q(z2)q(z3)*log(P(x1x2x3z1z2z3)/q(z1)q(z2)q(z3)))
= SIGMAz1z2z3(q(z1)q(z2)q(z3)*(logP(x1x2x3z1z2z3) - log(q(z1)q(z2)q(z3)))
= SIGMAz1z2z3(q(z1)q(z2)q(z3)*(logP(x1x2x3z1z2z3) - logq(z1)-logq(z2)-logq(z3))
Instead of solving them entirely, why not solve them one at a time? What if we
just solve for q(z1) and assume that q(z2) and q(z3) are known.
The math is complicated and I will not do it here.
ref: https://www.youtube.com/watch?v=OsunRTEh1pw&t=199s
