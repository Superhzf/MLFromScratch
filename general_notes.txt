1.Q: Why vectorization is faster than loops?
A:

2. Q: Difference between multiplication functions in NumPy?
A: np.multiply(a, b), element-wise multiplication
np.dot(a, b) == np.matmul(a, b) == a @ b, matrix multiplication

3. Q: What is the difference between NumPy max and NumPy maximum?
A: np.max only works on a single input array and finds the value of maximum
element in that entire array. Alternatively, it takes an axis argument and will
find the maximum value along axis of the input array.
np.maximum is to take two arrays and compute their element-wise maximum

4. Q: What is the relationship between OLS and gradient descent?
A: OLS is an analytical approach which means the problem has an closed form solution,
whereas gradient descent is an iterative method.

5. Q: Why is gradient descent often used compared to OLS?
A: Because 1. some problems do not have a closed form solution. 2. gradient descent
is computationally cheaper, in order to solve a problem using OLS, we may have to
calculate the matrix multiplication and invert matrix, they are very expensive.
3. gradient descent does not need to meet strict assumptions like no multicollinearity.

6. Q: Why is second-order optimization algorithms, such as Newton's method not as
widely used as stochastic gradient descent in machine learning problems?
A: TODO

7. I tested NAG optimization with the help of PyTorch (https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)
Assuming the PyTorch implement is correct, SGD with nesterov=True is very unstable. The loss changes drastically.
Whereas, the regular SGD with momentum works pretty well. So I decided not to implement
the NAG algorithm.
